/**
	Set or retrieve the default accuracy used for timers.
	uint64 timer_default_accuracy = MILLISECONDS;
	## Remarks:
		Internally NVGT always uses a clock with microsecond accuracy to track time, meaning that microseconds is the highest accuracy that timer objects support. However in many cases, microsecond accuracy is not desired and causes a user to need to type far too many 0s than they would desire when handling elapsed time. Therefor NVGT allows you to set a devisor/multiplier that is applied to any timer operation as it passes between NVGT and your script.
		The accuracy represented here is just a number of microseconds to divide values returned from timer.elapsed by before returning them to your script, as well as the number of microseconds to multiply by if one uses the timer.force function.
		If this property is changed, any timers that already exist will remain unaffected, and only newly created timers will use the updated value.
		Some useful constants are provided to make this property more useful:
		* MILLISECONDS: 1000 microseconds
		* SECONDS: 1000 MILLISECONDS
		* MINUTES: 60 SECONDS
		* HOURS: 60 MINUTES
		* DAYS: 24 HOURS.
*/

// Example:
void main() {
	timer_default_accuracy = SECONDS;
	alert("example", "about to wait for 1100ms");
	timer t;
	wait(1100);
	alert("example", t.elapsed); // Will show 1.
	timer_default_accuracy = 250; // You can set this to arbitrary values if you so wish.
	timer t2;
	wait(1); // or 1000 microseconds.
	alert("example", t2.elapsed); // Will display a number slightly higher than 4 as the wait call takes some extra microseconds to return.
}
